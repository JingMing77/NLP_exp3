{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## config"
      ],
      "metadata": {
        "id": "qKcmbAmC72DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "import threading\n",
        "\n",
        "\n",
        "def set_logger(config):\n",
        "    if not os.path.exists(config.log_path):\n",
        "        os.mkdir(config.log_path)\n",
        "    logging.basicConfig(\n",
        "        format='%(asctime)s %(levelname)-8s %(message)s',\n",
        "        level=logging.INFO,\n",
        "        datefmt='%Y-%m-%d %H:%M:%S',\n",
        "        filename=os.path.join(config.log_path, '{}.log'.format(config.model)),\n",
        "        filemode='a'\n",
        "    )\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
        "    console.setFormatter(formatter)\n",
        "    logging.getLogger('').addHandler(console)\n",
        "\n",
        "def load_file(fp: str, sep: str = None):\n",
        "    \"\"\"\n",
        "    读取文件；\n",
        "    若sep为None，按行读取，返回文件内容列表，格式为:[xxx,xxx,xxx,...]\n",
        "    若不为None，按行读取分隔，返回文件内容列表，格式为: [[xxx,xxx],[xxx,xxx],...]\n",
        "    \"\"\"\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "        if sep:\n",
        "            return [line.strip().split(sep) for line in lines]\n",
        "        else:\n",
        "            return lines\n",
        "\n",
        "def get_labels(config):\n",
        "    \"\"\"读取训练数据获取标签\"\"\"\n",
        "    labels = ['I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-PER', 'O', 'B-ORG']\n",
        "    labels.extend(['<START>', '<END>'])\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    _instance_lock = threading.Lock()\n",
        "    _init_flag = False\n",
        "\n",
        "    def __init__(self):\n",
        "        if not Config._init_flag:\n",
        "            Config._init_flag = True\n",
        "            self.base_path = os.path.abspath('/content/drive/MyDrive/Colab Notebooks')\n",
        "            self._init_train_config()\n",
        "\n",
        "\n",
        "    def _init_train_config(self):\n",
        "        self.label_list = []\n",
        "        self.use_gpu = True\n",
        "        self.device = \"cuda\"\n",
        "        self.checkpoints = False  # 使用预训练模型时设置为False\n",
        "        self.model = 'bert_bilstm_crf'  # 可选['bert_bilstm_crf','bilstm_crf','bilstm','crf','hmm']\n",
        "\n",
        "        # 输入数据集、日志、输出目录\n",
        "        self.train_file = os.path.join(self.base_path, 'data/train.txt')\n",
        "        self.test_file = os.path.join(self.base_path, 'data/test.txt')\n",
        "        self.log_path = os.path.join(self.base_path, 'logs')\n",
        "        # self.output_path = os.path.join(self.base_path, 'output', datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
        "        self.output_path = os.path.join(self.base_path, 'output', self.model)\n",
        "        self.trained_model_path = os.path.join(self.base_path, 'ckpts', self.model)\n",
        "        self.model_name_or_path = os.path.join(self.base_path, 'ckpts', 'bert-base-chinese') if not self.checkpoints \\\n",
        "            else self.trained_model_path\n",
        "\n",
        "        # 以下是模型训练参数\n",
        "        self.do_train = True\n",
        "        self.do_eval = False\n",
        "        self.need_birnn = True\n",
        "        self.do_lower_case = True\n",
        "        self.rnn_dim = 128\n",
        "        self.max_seq_length = 128\n",
        "        self.batch_size = 16\n",
        "        self.num_train_epochs = 2\n",
        "        self.ckpts_epoch = 1\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        self.learning_rate = 3e-5\n",
        "        self.adam_epsilon = 1e-8\n",
        "        self.warmup_steps = 0\n",
        "        self.logging_steps = 50\n",
        "        self.remove_O = False\n"
      ],
      "metadata": {
        "id": "cI-ZSAchypdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-crf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCIYN0gt79lJ",
        "outputId": "61ef0382-0095-4c63-f946-e88a62801643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT_BiLSTM_CRF 模型"
      ],
      "metadata": {
        "id": "slS6EEve8B2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from torchcrf import CRF\n",
        "\n",
        "\n",
        "class BERT_BiLSTM_CRF(BertPreTrainedModel):\n",
        "    def __init__(self, config, need_birnn=False, rnn_dim=128):\n",
        "        super(BERT_BiLSTM_CRF, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        out_dim = config.hidden_size  # 768\n",
        "\n",
        "        if need_birnn:\n",
        "            self.need_birnn = need_birnn\n",
        "            self.birnn = nn.LSTM(input_size=config.hidden_size, hidden_size=rnn_dim, num_layers=1, bidirectional=True,\n",
        "                                 batch_first=True)\n",
        "            out_dim = rnn_dim * 2\n",
        "\n",
        "        self.hidden2tag = nn.Linear(in_features=out_dim, out_features=config.num_labels)\n",
        "\n",
        "        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, tags, token_type_ids=None, attention_mask=None):\n",
        "        \"\"\"\n",
        "        :param input_ids:      torch.Size([batch_size,seq_len]), 代表输入实例的tensor张量\n",
        "        :param tags:      torch.Size([batch_size,seq_len]), 真实标签\n",
        "        :param token_type_ids: torch.Size([batch_size,seq_len]), 一个实例可以含有两个句子,相当于标记\n",
        "        :param attention_mask:     torch.Size([batch_size,seq_len]), 指定对哪些词进行self-Attention操作\n",
        "        :return: loss\n",
        "        \"\"\"\n",
        "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]  # torch.Size([batch_size,seq_len,hidden_size])\n",
        "        if self.need_birnn:\n",
        "            sequence_output, _ = self.birnn(sequence_output)  # (seq_length,batch_size,num_directions*hidden_size)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        emissions = self.hidden2tag(sequence_output)  # [seq_length, batch_size, num_labels]\n",
        "        loss = -1 * self.crf(emissions, tags, mask=attention_mask.byte())\n",
        "        return loss\n",
        "\n",
        "    def predict(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        \"\"\"\n",
        "        :param input_ids:      torch.Size([batch_size,seq_len]), 代表输入实例的tensor张量\n",
        "        :param token_type_ids:   torch.Size([batch_size,seq_len]), 一个实例可以含有两个句子,相当于标记\n",
        "        :param attention_mask:   torch.Size([batch_size,seq_len]), 指定对哪些词进行self-Attention操作\n",
        "\n",
        "        :return: pred_tags:     batch_size的list\n",
        "        \"\"\"\n",
        "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        if self.need_birnn:\n",
        "            sequence_output, _ = self.birnn(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        emissions = self.hidden2tag(sequence_output)\n",
        "        return self.crf.decode(emissions, attention_mask.byte())\n"
      ],
      "metadata": {
        "id": "nU-Zjnqt5F78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train and test"
      ],
      "metadata": {
        "id": "MpEYh7Jd9aiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader, SequentialSampler, Dataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertConfig\n",
        "\n",
        "# from utils import *\n",
        "# from dataloader import NERDataset\n",
        "# from models import BERT_BiLSTM_CRF\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class Metrics(object):\n",
        "    \"\"\"用于评价模型，计算每个标签的精确率，召回率，F1分数\"\"\"\n",
        "    def __init__(self, golden_tags, predict_tags, remove_O=False):\n",
        "        # 所有句子tags的拼接[[t1, t2], [t3, t4]...] --> [t1, t2, t3, t4...]\n",
        "        self.golden_tags = self.flatten_lists(golden_tags)\n",
        "        self.predict_tags = self.flatten_lists(predict_tags)\n",
        "        if remove_O:  # 将O标记移除，只关心实体标记\n",
        "            self._remove_Otags()\n",
        "\n",
        "        # 辅助计算的变量\n",
        "        self.tagset = set(self.golden_tags)\n",
        "        self.correct_tags_number = self.count_correct_tags()\n",
        "        self.predict_tags_counter = Counter(self.predict_tags)\n",
        "        self.golden_tags_counter = Counter(self.golden_tags)\n",
        "\n",
        "        self.precision_scores = self.cal_precision()\n",
        "        self.recall_scores = self.cal_recall()\n",
        "        self.f1_scores = self.cal_f1()\n",
        "\n",
        "    def flatten_lists(self, lists):\n",
        "        flatten_list = []\n",
        "        for l in lists:\n",
        "            if type(l) == list:\n",
        "                flatten_list += l\n",
        "            else:\n",
        "                flatten_list.append(l)\n",
        "        return flatten_list\n",
        "\n",
        "    def cal_precision(self):\n",
        "        precision_scores = {}\n",
        "        for tag in self.tagset:\n",
        "            precision_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
        "                self.predict_tags_counter[tag]\n",
        "\n",
        "        return precision_scores\n",
        "\n",
        "    def cal_recall(self):\n",
        "        recall_scores = {}\n",
        "        for tag in self.tagset:\n",
        "            recall_scores[tag] = self.correct_tags_number.get(tag, 0) / \\\n",
        "                self.golden_tags_counter[tag]\n",
        "        return recall_scores\n",
        "\n",
        "    def cal_f1(self):\n",
        "        f1_scores = {}\n",
        "        for tag in self.tagset:\n",
        "            p, r = self.precision_scores[tag], self.recall_scores[tag]\n",
        "            f1_scores[tag] = 2*p*r / (p+r+1e-10)  # 加上一个特别小的数，防止分母为0\n",
        "        return f1_scores\n",
        "\n",
        "    def report_scores(self):\n",
        "        \"\"\"将结果用表格的形式打印出来，像这个样子：\n",
        "                      precision    recall  f1-score   support\n",
        "              B-LOC      0.775     0.757     0.766      1084\n",
        "              I-LOC      0.601     0.631     0.616       325\n",
        "             B-MISC      0.698     0.499     0.582       339\n",
        "             I-MISC      0.644     0.567     0.603       557\n",
        "              B-ORG      0.795     0.801     0.798      1400\n",
        "              I-ORG      0.831     0.773     0.801      1104\n",
        "              B-PER      0.812     0.876     0.843       735\n",
        "              I-PER      0.873     0.931     0.901       634\n",
        "          avg/total      0.779     0.764     0.770      6178\n",
        "        \"\"\"\n",
        "        # 打印表头\n",
        "        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n",
        "        header = ['precision', 'recall', 'f1-score', 'support']\n",
        "        logging.info(header_format.format('', *header))\n",
        "\n",
        "        # 打印每个标签的 精确率、召回率、f1分数\n",
        "        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n",
        "        for tag in self.tagset:\n",
        "            logging.info(row_format.format(\n",
        "                tag,\n",
        "                self.precision_scores[tag],\n",
        "                self.recall_scores[tag],\n",
        "                self.f1_scores[tag],\n",
        "                self.golden_tags_counter[tag]\n",
        "            ))\n",
        "\n",
        "        # 计算并打印平均值\n",
        "        avg_metrics = self.cal_avg_metrics()\n",
        "        logging.info(row_format.format(\n",
        "            'avg/total',\n",
        "            avg_metrics['precision'],\n",
        "            avg_metrics['recall'],\n",
        "            avg_metrics['f1_score'],\n",
        "            len(self.golden_tags)\n",
        "        ))\n",
        "\n",
        "\n",
        "    def count_correct_tags(self):\n",
        "        \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n",
        "        correct_dict = {}\n",
        "        for gold_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
        "            if gold_tag == predict_tag:\n",
        "                if gold_tag not in correct_dict:\n",
        "                    correct_dict[gold_tag] = 1\n",
        "                else:\n",
        "                    correct_dict[gold_tag] += 1\n",
        "\n",
        "        return correct_dict\n",
        "\n",
        "    def cal_avg_metrics(self):\n",
        "        avg_metrics = {}\n",
        "        total = len(self.golden_tags)\n",
        "\n",
        "        avg_metrics['precision'] = 0.\n",
        "        avg_metrics['recall'] = 0.\n",
        "        avg_metrics['f1_score'] = 0.\n",
        "        for tag in self.tagset:\n",
        "            size = self.golden_tags_counter[tag]\n",
        "            avg_metrics['precision'] += self.precision_scores[tag] * size\n",
        "            avg_metrics['recall'] += self.recall_scores[tag] * size\n",
        "            avg_metrics['f1_score'] += self.f1_scores[tag] * size\n",
        "\n",
        "        for metric in avg_metrics.keys():\n",
        "            avg_metrics[metric] /= total\n",
        "\n",
        "        return avg_metrics\n",
        "\n",
        "    def _remove_Otags(self):\n",
        "        length = len(self.golden_tags)\n",
        "        O_tag_indices = [i for i in range(length) if self.golden_tags[i] == 'O']\n",
        "        self.golden_tags = [tag for i, tag in enumerate(self.golden_tags) if i not in O_tag_indices]\n",
        "        self.predict_tags = [tag for i, tag in enumerate(self.predict_tags) if i not in O_tag_indices]\n",
        "        logging.info(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n",
        "            length,\n",
        "            len(O_tag_indices),\n",
        "            len(O_tag_indices) / length * 100\n",
        "        ))\n",
        "\n",
        "    def report_confusion_matrix(self):\n",
        "        \"\"\"计算混淆矩阵\"\"\"\n",
        "        logging.info(\"Confusion Matrix:\")\n",
        "        tag_list = list(self.tagset)\n",
        "        # 初始化混淆矩阵 matrix[i][j]表示第i个tag被模型预测成第j个tag的次数\n",
        "        tags_size = len(tag_list)\n",
        "        matrix = []\n",
        "        for i in range(tags_size):\n",
        "            matrix.append([0] * tags_size)\n",
        "\n",
        "        for golden_tag, predict_tag in zip(self.golden_tags, self.predict_tags):\n",
        "            try:\n",
        "                row = tag_list.index(golden_tag)\n",
        "                col = tag_list.index(predict_tag)\n",
        "                matrix[row][col] += 1\n",
        "            except ValueError:  # 有极少数标记没有出现在golden_tags，但出现在predict_tags，跳过这些标记\n",
        "                continue\n",
        "\n",
        "        row_format_ = '{:>7} ' * (tags_size+1)\n",
        "        logging.info(row_format_.format(\"\", *tag_list))\n",
        "        for i, row in enumerate(matrix):\n",
        "            logging.info(row_format_.format(tag_list[i], *row))\n",
        "\n",
        "\n",
        "class Bert_Bilstm_Crf():\n",
        "    def __init__(self, config, device, use_gpu, n_gpu, writer, id2label):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.use_gpu = use_gpu\n",
        "        self.n_gpu = n_gpu\n",
        "        self.writer = writer\n",
        "        self.id2label = id2label\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(config.model_name_or_path,\n",
        "                                                  do_lower_case=True)\n",
        "        bert_config = BertConfig.from_pretrained(config.model_name_or_path, num_labels=len(config.label_list))\n",
        "        self.model = BERT_BiLSTM_CRF.from_pretrained(config.model_name_or_path, config=bert_config,\n",
        "                                                need_birnn=True, rnn_dim=config.rnn_dim)\n",
        "        self.model.to(device)\n",
        "        logging.info(\"loading tokenizer、bert_config and bert_bilstm_crf model successful!\")\n",
        "\n",
        "    def train(self):\n",
        "        if self.use_gpu and self.n_gpu > 1:\n",
        "            self.model = torch.nn.DataParallel(self.model)\n",
        "\n",
        "        logging.info(\"starting load train data and data_loader...\")\n",
        "        dataset = NERDataset(self.config, self.tokenizer, mode='train')\n",
        "        dataloader = DataLoader(dataset, self.config.batch_size, shuffle=True)\n",
        "\n",
        "        logging.info(\"loading train data_set and data_loader successful!\")\n",
        "\n",
        "        # 初始化模型参数优化器\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate, eps=self.config.adam_epsilon)\n",
        "\n",
        "        # 初始化学习率优化器\n",
        "        t_total = len(dataloader) // self.config.gradient_accumulation_steps * self.config.num_train_epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.config.warmup_steps,\n",
        "                                                    num_training_steps=t_total)\n",
        "        logging.info(\"loading AdamW optimizer、Warmup LinearSchedule and calculate optimizer parameter successful!\")\n",
        "\n",
        "        logging.info(\"====================== Running training ======================\")\n",
        "        logging.info(\n",
        "            f\"Num Examples:  {len(dataset)}, Num Batch Step: {len(dataloader)}, \"\n",
        "            f\"Num Epochs: {self.config.num_train_epochs}, Num scheduler steps：{t_total}\")\n",
        "\n",
        "        # 启用 BatchNormalization 和 Dropout\n",
        "        self.model.train()\n",
        "        global_step, tr_loss, logging_loss, best_f1 = 0, 0.0, 0.0, 0.0\n",
        "        for epoch in range(int(self.config.num_train_epochs)):\n",
        "            # model.train()\n",
        "            for batch, batch_data in enumerate(tqdm(dataloader, desc=\"Train_DataLoader\")):\n",
        "                # input_ids = torch.tensor(batch_data['input_ids'], dtype=torch.long)\n",
        "                # token_type_ids = torch.tensor(batch_data['token_type_ids'], dtype=torch.long)\n",
        "                # attention_mask = torch.tensor(batch_data['attention_mask'], dtype=torch.long)\n",
        "                # label_ids = torch.tensor(batch_data['label_ids'], dtype=torch.long)\n",
        "\n",
        "                batch_data = tuple(torch.stack(batch_data[k]).T.to(self.device) for k in batch_data.keys())\n",
        "                input_ids, token_type_ids, attention_mask, label_ids = batch_data\n",
        "                outputs = self.model(input_ids, label_ids, token_type_ids, attention_mask)\n",
        "                loss = outputs\n",
        "\n",
        "                if self.use_gpu and self.n_gpu > 1:\n",
        "                    loss = loss.mean()\n",
        "\n",
        "                if self.config.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.config.gradient_accumulation_steps\n",
        "\n",
        "                logging.info(f\"Epoch: {epoch}/{int(self.config.num_train_epochs)}\\tBatch: {batch}/{len(dataloader)}\\tLoss:{loss}\")\n",
        "                # 反向传播\n",
        "                loss.backward()\n",
        "                tr_loss += loss.item()\n",
        "\n",
        "                # 优化器_模型参数的总更新次数，和上面的t_total对应\n",
        "                if (batch + 1) % self.config.gradient_accumulation_steps == 0:\n",
        "                    # 更新参数\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    # 梯度清零\n",
        "                    self.model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if self.config.logging_steps > 0 and global_step % self.config.logging_steps == 0:\n",
        "                        tr_loss_avg = (tr_loss - logging_loss) / self.config.logging_steps\n",
        "                        self.writer.add_scalar(\"Train/loss\", tr_loss_avg, global_step)\n",
        "                        logging_loss = tr_loss\n",
        "\n",
        "            if self.config.do_eval:\n",
        "                logging.info(\"====================== Running Eval ======================\")\n",
        "                eval_data = NERDataset(self.config, self.tokenizer, mode=\"eval\")\n",
        "\n",
        "                avg_metrics, cal_indicators, eval_sens = self.evaluate(\n",
        "                    self.config, self.tokenizer, eval_data, self.model, self.id2label, self.device, tqdm_desc=\"Eval_DataLoader\")\n",
        "                f1_score = avg_metrics['f1_score']\n",
        "                self.writer.add_scalar(\"Eval/precision\", avg_metrics['precision'], epoch)\n",
        "                self.writer.add_scalar(\"Eval/recall\", avg_metrics['recall'], epoch)\n",
        "                self.writer.add_scalar(\"Eval/f1_score\", avg_metrics['f1_score'], epoch)\n",
        "\n",
        "                # save the best performs model\n",
        "                if f1_score > best_f1:\n",
        "                    logging.info(f\"******** the best f1 is {f1_score}, save model !!! ********\")\n",
        "                    best_f1 = f1_score\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "                    model_to_save.save_pretrained(self.config.trained_model_path)\n",
        "                    self.tokenizer.save_pretrained(self.config.trained_model_path)\n",
        "                    model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "                    model_to_save.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "                    self.tokenizer.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "\n",
        "            # # （如果config.do_eval=False，注释以下模型断点保存步骤）\n",
        "            # # 数据集过大，需要分阶段、分时训练时每隔一段时间保存checkpoints\n",
        "            # if (epoch + 1) % self.config.ckpts_epoch == 0:\n",
        "            #     model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "            #     model_to_save.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "            #     self.tokenizer.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "        model_to_save.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "        self.tokenizer.save_pretrained(os.path.join(self.config.trained_model_path, 'checkpoints'))\n",
        "\n",
        "        # torch.save(self.config, os.path.join(self.config.trained_model_path, 'training_config.bin'))\n",
        "        # torch.save(self.model, os.path.join(self.config.trained_model_path, 'ner_model.ckpt'))\n",
        "        # logging.info(\"training_args.bin and ner_model.ckpt save successful!\")\n",
        "\n",
        "        self.writer.close()\n",
        "        logging.info(\"NER model training successful!!!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate(config, tokenizer, dataset, model, id2label, device, tqdm_desc):\n",
        "        sampler = SequentialSampler(dataset)\n",
        "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=config.batch_size)\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model = model.module\n",
        "        model.eval()\n",
        "\n",
        "        id2label[-1] = 'NULL'  # 解码临时添加\n",
        "        ori_tokens = [tokenizer.decode(tdt['input_ids']).split(\" \") for tdt in dataset]\n",
        "        ori_labels = [[id2label[idx] for idx in tdt['label_ids']] for tdt in dataset]\n",
        "        pred_labels = []\n",
        "\n",
        "        for b_i, batch_data in enumerate(tqdm(data_loader, desc=tqdm_desc)):\n",
        "            batch_data = tuple(torch.stack(batch_data[k]).T.to(device) for k in batch_data.keys())\n",
        "            input_ids, token_type_ids, attention_mask, label_ids = batch_data\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model.predict(input_ids, token_type_ids, attention_mask)\n",
        "\n",
        "            for logit in logits:\n",
        "                pred_labels.append([id2label[idx] for idx in logit])\n",
        "\n",
        "        assert len(pred_labels) == len(ori_tokens) == len(ori_labels)\n",
        "        eval_sens = []\n",
        "        for ori_token, ori_label, pred_label in zip(ori_tokens, ori_labels, pred_labels):\n",
        "            sen_tll = []\n",
        "            for ot, ol, pl in zip(ori_token, ori_label, pred_label):\n",
        "                if ot in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "                    continue\n",
        "                sen_tll.append((ot, ol, pl))\n",
        "            eval_sens.append(sen_tll)\n",
        "\n",
        "        golden_tags = [[ttl[1] for ttl in sen] for sen in eval_sens]\n",
        "        predict_tags = [[ttl[2] for ttl in sen] for sen in eval_sens]\n",
        "        cal_indicators = Metrics(golden_tags, predict_tags, remove_O=config.remove_O)\n",
        "        avg_metrics = cal_indicators.cal_avg_metrics()  # avg_metrics['precision'], avg_metrics['recall'], avg_metrics['f1_score']\n",
        "\n",
        "        return avg_metrics, cal_indicators, eval_sens\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        logging.info(\"====================== Running test ======================\")\n",
        "        dataset = NERDataset(self.config, self.tokenizer, mode='test')\n",
        "        avg_metrics, cal_indicators, eval_sens = self.evaluate(\n",
        "            self.config, self.tokenizer, dataset, self.model, self.id2label, self.device, tqdm_desc=\"Test_DataLoader\")\n",
        "\n",
        "        cal_indicators.report_scores()  # avg_metrics['precision'], avg_metrics['recall'], avg_metrics['f1_score']\n",
        "        cal_indicators.report_confusion_matrix()\n",
        "        # 将测试结果写入本地\n",
        "        with open(os.path.join(self.config.output_path, \"token_labels_test.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            for sen in eval_sens:\n",
        "                for ttl in sen:\n",
        "                    f.write(f\"{ttl[0]}\\t{ttl[1]}\\t{ttl[2]}\\n\")\n",
        "                f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "p4ns4tSgmxSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loader"
      ],
      "metadata": {
        "id": "TWM01QbM9g3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "class InputData(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "    def __init__(self, input_ids, token_type_ids, attention_mask, label_id):\n",
        "        \"\"\"\n",
        "        :param input_ids:       单词在词典中的编码\n",
        "        :param attention_mask:  指定 对哪些词 进行self-Attention操作\n",
        "        :param token_type_ids:  区分两个句子的编码（上句全为0，下句全为1）\n",
        "        :param label_id:        标签的id\n",
        "        \"\"\"\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, config, tokenizer, mode=\"train\"):\n",
        "        # text: a list of words, all text from the training dataset\n",
        "        super(NERDataset, self).__init__()\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        if mode == \"train\":\n",
        "            self.file_path = config.train_file\n",
        "        elif mode == \"test\":\n",
        "            self.file_path = config.test_file\n",
        "        elif mode == \"eval\":\n",
        "            self.file_path = config.dev_file\n",
        "        else:\n",
        "            raise ValueError(\"mode must be one of train, or test\")\n",
        "\n",
        "        self.tdt_data = self.get_data()\n",
        "        self.len = len(self.tdt_data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        对指定数据集进行预处理，进一步封装数据，包括:\n",
        "        tdt_data：[InputData(guid=index, text=text, label=label)]\n",
        "        feature：BatchEncoding( input_ids=input_ids,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                label_id=label_ids)\n",
        "        data_f： 处理完成的数据集, TensorDataset(all_input_ids, all_token_type_ids, all_attention_mask, all_label_ids)\n",
        "        \"\"\"\n",
        "        label_map = {label: i for i, label in enumerate(self.config.label_list)}\n",
        "        max_seq_length = self.config.max_seq_length\n",
        "\n",
        "        data = self.tdt_data[idx]\n",
        "        data_text_list = data.text.split(\" \")\n",
        "        data_label_list = data.label.split(\" \")\n",
        "        assert len(data_text_list) == len(data_label_list)\n",
        "\n",
        "        features = self.tokenizer(''.join(data_text_list), padding='max_length', max_length=max_seq_length, truncation=True)\n",
        "        label_ids = [label_map[label] for label in data_label_list]\n",
        "        label_ids = [label_map[\"<START>\"]] + label_ids + [label_map[\"<END>\"]]\n",
        "        while len(label_ids) < max_seq_length:\n",
        "            label_ids.append(-1)\n",
        "        features.data['label_ids'] = label_ids\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "    def read_file(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines, words, labels = [], [], []\n",
        "            for line in f.readlines():\n",
        "                contends = line.strip()\n",
        "                tokens = line.strip().split()\n",
        "                if len(tokens) == 2:\n",
        "                    words.append(tokens[0])\n",
        "                    labels.append(tokens[1])\n",
        "                else:\n",
        "                    if len(contends) == 0 and len(words) > 0:\n",
        "                        label, word = [], []\n",
        "                        for l, w in zip(labels, words):\n",
        "                            if len(l) > 0 and len(w) > 0:\n",
        "                                label.append(l)\n",
        "                                word.append(w)\n",
        "                        lines.append([' '.join(label), ' '.join(word)])\n",
        "                        words, labels = [], []\n",
        "        return lines\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "        '''数据预处理并返回相关数据'''\n",
        "        lines = self.read_file()\n",
        "        tdt_data = []\n",
        "        for i, line in enumerate(lines):\n",
        "            guid = str(i)\n",
        "            text = line[1]\n",
        "            # word_piece = self.word_piece_bool(text)\n",
        "            # if word_piece:\n",
        "            #     continue\n",
        "            label = line[0]\n",
        "            tdt_data.append(InputData(guid=guid, text=text, label=label))\n",
        "\n",
        "        return tdt_data\n",
        "\n",
        "\n",
        "    def word_piece_bool(self, text):\n",
        "        word_piece = False\n",
        "        data_text_list = text.split(' ')\n",
        "        for i, word in enumerate(data_text_list):\n",
        "            # 防止wordPiece情况出现，不过貌似不会\n",
        "            token = self.tokenizer.tokenize(word)\n",
        "            # 单个字符表示不会出现wordPiece\n",
        "            if len(token) != 1:\n",
        "                word_piece = True\n",
        "\n",
        "        return word_piece\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_data_to_features(self, tdt_data):\n",
        "        \"\"\"\n",
        "        对输入数据进行特征转换\n",
        "        例如:\n",
        "            guid: 0\n",
        "            tokens: [CLS] 王 辉 生 前 驾 驶 机 械 洒 药 消 毒 9 0 后 王 辉 ， 2 0 1 0 年 1 2 月 参 军 ， 2 0 1 5 年 1 2 月 退 伍 后 ， 先 是 应 聘 当 辅 警 ， 后 来 在 父 亲 成 立 的 扶 风 恒 盛 科 [SEP]\n",
        "            input_ids: 101 4374 6778 4495 1184 7730 7724 3322 3462 3818 5790 3867 3681 130 121 1400 4374 6778 8024 123 121 122 121 2399 122 123 3299 1346 1092 8024 123 121 122 126 2399 122 123 3299 6842 824 1400 8024 1044 3221 2418 5470 2496 6774 6356 8024 1400 3341 1762 4266 779 2768 4989 4638 2820 7599 2608 4670 4906 102\n",
        "            token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "            attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
        "            label_ids: 2 5 3 2 2 2 2 2 2 2 2 2 2 4 11 11 5 3 2 4 11 11 11 11 11 11 11 2 2 2 4 11 11 11 11 11 11 11 2 2 2 2 2 2 2 2 2 0 14 2 2 2 2 2 2 2 2 2 12 7 7 7 7 2\n",
        "        \"\"\"\n",
        "        label_map = {label: i for i, label in enumerate(self.config.label_list)}\n",
        "        max_seq_length = self.config.max_seq_length\n",
        "\n",
        "        features = []\n",
        "        for data in tdt_data:\n",
        "            data_text_list = data.text.split(\" \")\n",
        "            data_label_list = data.label.split(\" \")\n",
        "            assert len(data_text_list) == len(data_label_list)\n",
        "\n",
        "            tokens, labels, ori_tokens = [], [], []\n",
        "            word_piece = False\n",
        "            for i, word in enumerate(data_text_list):\n",
        "                # 防止wordPiece情况出现，不过貌似不会\n",
        "                token = self.tokenizer.tokenize(word)\n",
        "                tokens.extend(token)\n",
        "                label = data_label_list[i]\n",
        "                ori_tokens.append(word)\n",
        "                # 单个字符不会出现wordPiece\n",
        "                if len(token) == 1:\n",
        "                    labels.append(label)\n",
        "                else:\n",
        "                    word_piece = True\n",
        "\n",
        "            if word_piece:\n",
        "                logging.info(\"Error tokens!!! skip this lines, the content is: %s\" % \" \".join(data_text_list))\n",
        "                continue\n",
        "\n",
        "            assert len(tokens) == len(ori_tokens)\n",
        "\n",
        "            # feature = self.tokenizer(''.join(tokens), padding='max_length', max_length=max_seq_length, truncation=True)\n",
        "            # label_ids = [label_map[label] for label in labels]\n",
        "            # label_ids = [label_map[\"<START>\"]] + label_ids + [label_map[\"<END>\"]]\n",
        "            # while len(label_ids) < max_seq_length:\n",
        "            #     label_ids.append(-1)\n",
        "            # feature.data['label_ids'] = label_ids\n",
        "            # features.append(feature)\n",
        "\n",
        "            if len(tokens) >= max_seq_length - 1:\n",
        "                # -2的原因是因为序列需要加一个句首和句尾标志\n",
        "                tokens = tokens[0:(max_seq_length - 2)]\n",
        "                labels = labels[0:(max_seq_length - 2)]\n",
        "\n",
        "            label_ids = [label_map[label] for label in labels]\n",
        "            new_tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "            input_ids = self.tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "            token_type_ids = [0] * len(input_ids)\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "            label_ids = [label_map[\"<START>\"]] + label_ids + [label_map[\"<END>\"]]\n",
        "\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                token_type_ids.append(0)\n",
        "                label_ids.append(0)\n",
        "\n",
        "            features.append(InputFeatures(input_ids=input_ids,\n",
        "                                          token_type_ids=token_type_ids,\n",
        "                                          attention_mask=attention_mask,\n",
        "                                          label_id=label_ids))\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "RCyO0kKt9NJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "fxnYEqnA9nir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# from utils import *\n",
        "# from trainer import Bert_Bilstm_Crf\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = Config()\n",
        "    set_logger(config)\n",
        "    writer = SummaryWriter(log_dir=os.path.join(config.output_path, \"visual\"), comment=\"ner\")\n",
        "\n",
        "    if config.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(config.gradient_accumulation_steps))\n",
        "\n",
        "    use_gpu = torch.cuda.is_available() and config.use_gpu\n",
        "    device = torch.device('cuda' if use_gpu else 'cpu')\n",
        "    config.device = device\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    logging.info(f\"available device: {device}，count_gpu: {n_gpu}\")\n",
        "\n",
        "    config.label_list = get_labels(config)\n",
        "    label2id = {label: i for i, label in enumerate(config.label_list)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "    logging.info(\"loading label2id and id2label dictionary successful!\")\n",
        "\n",
        "    # Bert_Bilstm_Crf模型的训练与测试\n",
        "    trainer_bbc = Bert_Bilstm_Crf(config, device, use_gpu, n_gpu, writer, id2label)\n",
        "    # trainer_bbc.train() # 训练\n",
        "    trainer_bbc.test()  # 测试\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_xAV_Mfqn4A",
        "outputId": "efc6e5fd-6ca5-4d43-8316-bd191ef6bb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BERT_BiLSTM_CRF were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/ckpts/bert-base-chinese and are newly initialized: ['birnn.bias_hh_l0_reverse', 'hidden2tag.weight', 'birnn.bias_hh_l0', 'crf.end_transitions', 'birnn.weight_hh_l0_reverse', 'birnn.weight_ih_l0_reverse', 'birnn.bias_ih_l0_reverse', 'crf.start_transitions', 'crf.transitions', 'birnn.weight_ih_l0', 'hidden2tag.bias', 'birnn.bias_ih_l0', 'birnn.weight_hh_l0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Train_DataLoader:   0%|          | 0/3167 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:519.)\n",
            "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n",
            "Train_DataLoader: 100%|██████████| 3167/3167 [24:42<00:00,  2.14it/s]\n",
            "Train_DataLoader: 100%|██████████| 3167/3167 [24:55<00:00,  2.12it/s]\n",
            "Removed shared tensor {'birnn.bias_hh_l0_reverse', 'birnn.bias_hh_l0', 'birnn.weight_hh_l0_reverse', 'birnn.weight_ih_l0_reverse', 'birnn.bias_ih_l0_reverse', 'birnn.bias_ih_l0', 'birnn.weight_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Test_DataLoader: 100%|██████████| 290/290 [00:43<00:00,  6.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预测"
      ],
      "metadata": {
        "id": "7AqLzaSw-Xxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "txt_path = '/content/drive/MyDrive/Colab Notebooks/output/bert_bilstm_crf/token_labels_test.txt'\n",
        "preds = []\n",
        "labels = []\n",
        "texts = []\n",
        "error = 0\n",
        "with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        t = line.split()\n",
        "        if len(t) == 3:\n",
        "            if t[2] not in ['I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-PER', 'O', 'B-ORG']:\n",
        "                error += 1\n",
        "                continue\n",
        "            texts.append(t[0])\n",
        "            labels.append(t[1])\n",
        "            preds.append(t[2])\n",
        "precision = precision_score(labels, preds, average='macro')\n",
        "recall = recall_score(labels, preds, average='macro')\n",
        "f1 = f1_score(labels, preds, average='macro')\n",
        "report = classification_report(labels, preds)\n",
        "print()\n",
        "print(report)\n",
        "print()\n",
        "print('precision: ', precision)\n",
        "print('recall: ', recall)\n",
        "print('f1_score: ', f1)\n",
        "print('error: ', error)\n",
        "print()\n",
        "print('原文：', texts[:16])\n",
        "print('标签：', labels[:16])\n",
        "print('预测：', preds[:16])"
      ],
      "metadata": {
        "id": "2BmZhOfiVW94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cf1319-29f9-4f3b-f60f-53dad698ac47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.97      0.94      0.95      2871\n",
            "       B-ORG       0.92      0.93      0.92      1327\n",
            "       B-PER       0.97      0.97      0.97      1972\n",
            "       I-LOC       0.95      0.93      0.94      4370\n",
            "       I-ORG       0.93      0.96      0.95      5640\n",
            "       I-PER       0.98      0.98      0.98      3844\n",
            "           O       1.00      1.00      1.00    150935\n",
            "\n",
            "    accuracy                           0.99    170959\n",
            "   macro avg       0.96      0.96      0.96    170959\n",
            "weighted avg       0.99      0.99      0.99    170959\n",
            "\n",
            "\n",
            "precision:  0.9584684531096469\n",
            "recall:  0.9576453856701352\n",
            "f1_score:  0.9579837850878423\n",
            "error:  1\n",
            "\n",
            "原文： ['中', '共', '中', '央', '致', '中', '国', '致', '公', '党', '十', '一', '大', '的', '贺', '词']\n",
            "标签： ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O']\n",
            "预测： ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQC5lg9frqST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQXHrybAtxGb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}